import torch 

print("ðŸ”¥ CUDA available:", torch.cuda.is_available())
print("ðŸ”¥ Current GPU memory allocated:", torch.cuda.memory_allocated() // 1024**2, "MB")
print("ðŸ”¥ Current GPU memory reserved:", torch.cuda.memory_reserved() // 1024**2, "MB")

